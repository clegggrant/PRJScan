Hindawi Journal of Ophthalmology Volume 2018, Article ID 1875431, 6 pages https://doi.org/10.1155/2018/1875431 Research Article Deep Neural Network-Based Method for Detecting Central Retinal Vein Occlusion Using Ultrawide-Field Fundus Ophthalmoscopy Daisuke Nagasato ,1 Hitoshi Tabuchi,1 Hideharu Ohsugi,1 Hiroki Masumoto,1 Hiroki Enno,2 Naofumi Ishitobi,1 Tomoaki Sonobe,1 Masahiro Kameoka,1 Masanori Niki,3 Ken Hayashi,4 and Yoshinori Mitamura 1Department of Ophthalmology, Tsukazaki Hospital, Himeji, Japan 2Rist Inc., Tokyo, Japan 3Department of Ophthalmology, Institute of Biomedical Sciences, Tokushima University Graduate School, Tokushima, Japan 4Hayashi Eye Hospital, Fukuoka, Japan 3 Correspondence should be addressed to Daisuke Nagasato; d.nagasato@tsukazaki-eye.net Received 5 September 2018; Accepted 17 October 2018; Published 1 November 2018 Academic Editor: Elad Moisseiev Copyright © 2018 Daisuke Nagasato et al. +is is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. +e aim of this study is to assess the performance of two machine-learning technologies, namely, deep learning (DL) and support vector machine (SVM) algorithms, for detecting central retinal vein occlusion (CRVO) in ultrawide-ﬁeld fundus images. Images from 125 CRVO patients (n � 125 images) and 202 non-CRVO normal subjects (n � 238 images) were included in this study. Training to construct the DL model using deep convolutional neural network algorithms was provided using ultrawide-ﬁeld fundus images. +e SVM uses scikit-learn library with a radial basis function kernel. +e diagnostic abilities of DL and the SVM were compared by assessing their sensitivity, speciﬁcity, and area under the curve (AUC) of the receiver operating characteristic curve for CRVO. For diagnosing CRVO, the DL model had a sensitivity of 98.4% (95% conﬁdence interval (CI), 94.3–99.8%) and a speciﬁcity of 97.9% (95% CI, 94.6–99.1%) with an AUC of 0.989 (95% CI, 0.980–0.999). In contrast, the SVM model had a sensitivity of 84.0% (95% CI, 76.3–89.3%) and a speciﬁcity of 87.5% (95% CI, 82.7–91.1%) with an AUC of 0.895 (95% CI, 0.859–0.931). +us, the DL model outperformed the SVM model in all indices assessed (P< 0.001 for all). Our data suggest that a DL model derived using ultrawide-ﬁeld fundus images could distinguish between normal and CRVO images with a high level of accuracy and that automatic CRVO detection in ultrawide-ﬁeld fundus ophthalmoscopy is possible. +is proposed DL-based model can also be used in ultrawide-ﬁeld fundus ophthalmoscopy to accurately diagnose CRVO and improve medical care in remote locations where it is diﬃcult for patients to attend an ophthalmic medical center. 1. Introduction Central retinal vein occlusion (CRVO) is a vascular disease of the eye and a known cause of signiﬁcant visual morbidity, including sudden blindness [1]. Pathogenesis of CRVO is believed to follow the principles of Virchow’s triad of thrombogenesis, namely, vessel damage, stasis, and hyper- coagulability [2]. In CRVO, the fundus may show retinal hemorrhages, dilated tortuous retinal veins, cotton-wool spots, optic edema, and macular edema (ME); ME is the most important cause of visual impairment in CRVO [3]. Intravitreous injections of antivascular endothelial growth factor (VEGF) agents have been shown to signiﬁcantly improve visual acuity in eyes with CRVO-associated ME [4]. However, any delay in treatment with anti-VEGF agents results in poor functional improvement, and it is diﬃcult to subsequently achieve satisfactory improvement in vision [5–7]. +us, it is important to treat CRVO patients in an ophthalmic specialty center immediately after the onset to preserve visual function. However, establishing a large number of such centers is impractical because of rising 2 Journal of Ophthalmology public healthcare costs, a problem that is burdening several nations worldwide [8]. Recent remarkable advances in medical equipment include the ultrawide-ﬁeld scanning laser ophthalmo- scope, the Optos 200T× (Optos PLC, Dunfermline, United Kingdom). +e Optos can easily and noninvasively pro- vide wide-ﬁeld fundus images (Figure 1) without mydriatic agent use, and it has been used for diagnosing or moni- toring multiple conditions and for treatment evaluation in peripheral retinal and vascular pathology [9]. Importantly, if pupillary block and elevated intraocular pressure as- sociated with dilation can be avoided, a trained non- medical personnel can safely capture images to use them in telemedicine applications, especially in areas without ophthalmologists. Image processing approaches using two machine- learning algorithms, namely, deep learning (DL) and sup- port vector machines (SVMs), have retained investigator attention for years because of their extremely high- performance levels; in fact, increasing number of studies have assessed their applications in medical imaging [10–14]. Nonetheless, in ophthalmology, the use of image processing technology that uses DL algorithms and SVM models to analyze medical images has been previously reported [13, 15, 16]. However, to the best of our knowledge, no study has evaluated the possibility of automated CRVO diagnosis using Optos images and machine-learning technology. +erefore, in this study, we assessed the ability of a DL model to detect CRVO using Optos images and compared the results between DL- and SVM-based algorithms. 2. Materials and Methods 2.1. Image Dataset. Optos images of patients with acute CRVO and those without fundus diseases were extracted from the clinical database of the ophthalmology de- partments of the Tsukazaki Hospital, Tokushima University Hospital, and Hayashi Eye Hospital. +ese images were reviewed by a retinal specialist and stored in an analytical database. Of the 363 fundus images selected, 125 were from CRVO patients and 238 were from non-CRVO healthy subjects. data were divided into K groups, and (K− 1) groups were We used K-fold cross validation in this study, and it has been described in detail elsewhere [17, 18]. Brieﬂy, image used as training data, whereas one data group was used for validation. +is process was repeated K times until each of the K groups became a validation dataset. +e number of groups (K) was calculated using Sturges’ formula (K � 1 + log2 N). Sturges’ formula is used to decide the number of classes in the histogram [19, 20]. +us, in this study, we categorized the data into nine groups. Images in the training dataset were augmented by adjusting for brightness, gamma correction, histogram equalization, noise addition, and inversion so that the amount of training data increased by 18-fold. +e deep convolutional neural network (DNN) model, as detailed below, was created and was trained using preprocessed image data. +is study was conducted in compliance with the principles of the Declaration of Helsinki and was approved by the ethics committees of Tsukazaki Hospital, Tokushima University Hospital, and Hayashi Eye Hospital. 2.2. Deep LearningModelandTraining. A DNN model called the Visual Geometry Group-16 (VGG-16) [21] was used in the present study, and its schematic is shown in Figure 2. +is type of DNN is conﬁgured to automatically learn local features of images and generate a classiﬁcation model [22–24]. +e aspect ratio of the original Optos images was 3,900 × 3,072 pixels; however, for analysis, we changed the aspect ratio of all input images and resized them to 256 × 192 pixels. As the RGB input of images had a range of 0–255, it was ﬁrst normalized to a range of 0–1 by dividing it by 255. +e VGG-16 model comprises ﬁve blocks and three fully connected layers. Each block includes convolutional layers followed by a max-pooling layer with decreasing position sensitivity but greater generic recognition [25]. Flattening of the output from block 5 results in only two fully connected layers. +e ﬁrst layer removes spatial information from the extracted feature vectors, and the second layer is a classiﬁ- cation layer that uses feature vectors from target images acquired in previous layers in combination with the softmax function for binary classiﬁcation. To improve generalization performance, dropout processing was performed such that masking was achieved with a probability of 25% in the ﬁrst fully connected layer. Fine tuning was used to increase the learning speed and achieve higher performance with lower quantitates of data [26, 27]. We used the following parameters from ImageNet: blocks 1 to 4 were ﬁxed, whereas block 5 and the fully connected layers were trained. +e weights of block 5 and the fully connected layers were updated using the optimization momentum stochastic gra- dient descent algorithm (learning coeﬃcient � 0.0005, inertial term � 0.9) [28, 29]. Of the 40 DL models obtained in 40 learning cycles, the one with the highest rate of correct an- swers for the test data was selected as the DL model to be evaluated in this study. For this purpose, Keras (https:// keras.io/ja/) was run on TensorFlow (https://www.tensorﬂow. org/) written in Python and was used to build and evaluate the model. We trained the model using the CPU of Core (TM) i7-8700K by Intel and the GPU of GeForce GTX 1080 Ti by NVIDIA. 2.3. Support Vector Machine Model. We used the soft- margin SVM implemented in the scikit-learn library using the radial basis function kernel [30]. We reduced all images to 60 dimensions as this was the number of dimensions that was found to provide the highest correct answer rate for the test data; for this, we tested 10–70 dimensions in steps of 10. +e optimal values for cost parameter “C” of the SVM al- gorithm and parameter “c” of the radial basis function were determined by grid search using quadrant cross validation, and the combination with the highest average correct answer rate was selected. +e parameter values tested for C were 1, 10, 100, and 1000 and those for c were 0.0001, 0.001, 0.01, Journal of Ophthalmology 3 Figure 1: Representative fundus images obtained using ultrawide-ﬁeld scanning laser ophthalmoscopy. Ultrawide-ﬁeld fundus images of the right eye without central retinal vein occlusion (CRVO) (A) and with CRVO (B). Figure 2: Overall architecture of Visual Geometry Group-16 model. Visual Geometry Group-16 (VGG-16) comprises ﬁve blocks and three fully connected layers. Each block includes convolutional layers followed by a max-pooling layer. Flattening of the output matrix after block 5 resulted in two fully connected layers for binary classiﬁcation. +e deep convolutional neural network used ImageNet parameters; the weights of blocks 1–4 were ﬁxed, whereas the weights of block 5 and the fully connected layers were adjusted. 0.1, and 1. +e ﬁnal learning model was generated using the optimized parameter values of C � 10 and c � 0.0001. modiﬁer. +is process was performed using Python Keras- vis (https://raghakot.github.io/keras-vis/). 2.4. Outcomes. Receiver operating characteristic (ROC) curves for CRVO were created on the basis of the ability of the DL and SVM models to distinguish between CRVO and non-CRVO images, and the models were compared using area under the curve (AUC), sensitivity, and speciﬁcity values. 2.5. Heat Map. A heat map of the DNN focus site was created and classiﬁed using gradient-weighted class acti- vation mapping [21]. Next, composite images were created by overlaying heat maps of the DNN focus site on the corresponding CRVO and non-CRVO images. +e third convolution layer in block 3 was deﬁned as the target layer, and the rectiﬁed linear unit was used as the backprop 2.6. Statistical Analysis. Patient demographic data such as age were compared using Student’s t-test, whereas Fisher’s exact test was used for comparing the gender ratio and the ratio of the right to left eye images. +e 95% conﬁdence interval (CI) of AUC was obtained as follows. Images judged to exceed a threshold were de- ﬁned as positive for CRVO, and the ROC curve was created. We created nine such models and nine ROC curves. For determining AUC, the 95% CI was obtained by assuming normal distribution and using the average and standard deviation of the nine ROC curves. For estimating sensitivity and speciﬁcity, optimal cutoﬀ values, which are the points closest to the point at which both sensitivity and speciﬁcity are 100% in each ROC curve, were used [26]. +e sensitivity and speciﬁcity at the optimal cutoﬀ value were calculated 3 × 3 conv, 643 × 3 conv, 64Max poolingBlock 1Block 2Block 3Block 5Block 4Max poolingFlattenFc 256Fc 23 × 3 conv, 2563 × 3 conv, 2563 × 3 conv, 256Max pooling3 × 3 conv, 5123 × 3 conv, 5123 × 3 conv, 512Max pooling3 × 3 conv, 5123 × 3 conv, 5123 × 3 conv, 5123 × 3 conv, 1283 × 3 conv, 128Max pooling 4 Journal of Ophthalmology using the Youden index [31]. +e ROC curve was calculated using scikit-learn, and CIs for sensitivity and speciﬁcity were determined using SciPy. +e paired t-test was used to compare AUCs between the DL and the SVM models. 3. Results We used 125 CRVO images from 125 patients (mean age, 67.8± 13.9 years; 67 men and 58 women; 61 left fundus and 202 subjects (mean age, 68.6± 7.9 years; 104 men and 98 64 right fundus images) and 238 non-CRVO images from women; 122 left fundus and 116 right fundus images) in this analysis. No signiﬁcant diﬀerences were detected between these two groups with respect to age, gender ratio, and left- right eye image ratio (Table 1). +e DL model’s sensitivity for diagnosing CRVO was 98.4% (95% CI, 94.3–99.8%), its speciﬁcity was 97.9% (95% CI, 94.6–99.1%), and the AUC was 0.989 (95% CI, 0.980– 0.999); in contrast, sensitivity of the SVM model was 84.0% (95% CI, 76.3–89.3%), its speciﬁcity was 87.5% (95% CI, 82.7–91.1%), and the AUC was 0.895 (95% CI, 0.859–0.931). In ROC curves, AUC of the DL model was signiﬁcantly higher than that of the SVM model (P< 0.001) (Figure 3). A composite image, comprising the fundal image superimposed with its corresponding heat map, was created by the DNN, and these images showed that DNNs could accurately identify crucial areas in the fundal images; a representative composite image is presented in Figure 4. Blue was used to indicate the strength of DNN-based identiﬁcation, and an increase in color intensity was ob- served in areas with retinal hemorrhage and at the focus points. +us, in non-CRVO images, the heat map showed that focal points accumulated around the optic disc, whereas in CRVO images, focal points accumulated around the optic disc and around retinal hemorrhages. +ese results imply that DNNs may be able to distinguish between CRVO eyes and normal eyes by identifying and highlighting retinal hemorrhages. 4. Discussion +e fundamental aim of this study was to explore the possibility of early detection of CRVO from Optos fundus photographs using DL-based algorithms. If screening for CRVO is possible noninvasively and without the use of mydriatic agents, this approach would be medically viable. Currently, it is unreasonable to expect ophthalmologists to interpret all Optos-acquired fundus images because of as- sociated medical resource costs. +erefore, a DL model that can accurately diagnose conditions based on ultrawide-ﬁeld fundus ophthalmoscopy images without the need for human input can be used to screen and diagnose a very large number of patients at a very low cost. Here, we have used DL technology to identify Optos images that show presence of CRVO. Our results show that the DL model has higher sensitivity, speciﬁcity, and AUC values than the SVM model for detecting CRVO in Optos- derived fundus photographs. Table 1: Patient demographics. CRVO 67.8± 13.9 125 (125) Non-CRVO 68.6± 7.9 238 (202) Number of images (patients) Age (yrs) Sex, female 58 (46.4%) 98 (48.5%) Left fundus 61 (48.8%) 122 (51.3%) p value — 0.489 (Student’s t-test) 0.734 (Fisher’s exact test) 0.660 (Fisher’s exact test) Figure 3: Receiver operating characteristic (ROC) curve for central retinal vein occlusion. Further, using heat maps, we show that DNN could accurately identify an area around the optic disc in the non- CRVO images, whereas in CRVO images, it focused on the area around the optic disc and could highlight retinal hemorrhages. +is result implies that the proposed DNN model may be able to identify CRVO by focusing on areas with suspected retinal hemorrhages. It is known that DL algorithm-based models can automatically learn local fea- ture values of images and generate classiﬁcation models [22, 26, 29, 31]. Additionally, DL includes several layers for the identiﬁcation of local features of complicated diﬀerences, which can subsequently be combined [29]. In recent years, a number of studies have addressed that CNN hugely outperforms classic ML algorithms in image classiﬁcation tasks [16, 32–34]. Recently, Wang et al. have reported that the performance of the DL model was not signiﬁcantly diﬀerent from that of the best classical methods, including SVM and human physicians, when classifying mediastinal lymph node metastasis in nonsmall cell lung cancer using positron emission tomography/computed to- mography images [35]. +is could be because image in- formation necessary for classiﬁcation was lost during image convolution in DL. In contrast, we found that the perfor- mance of the DL model was better than that of the SVM model in accurately diagnosing CRVO using Optos images. As most cases of CRVO need early intervention, patients diagnosed with CRVO using this method can immediately consult retinal specialists and receive necessary advanced Deep learningSVM1–specificitySensitivity110.80.80.60.60.40.40.20.200 Journal of Ophthalmology 5 Figure 4: Representative ultrawide-ﬁeld fundus images and corresponding heat maps. +e ultrawide-ﬁeld fundus image without central retinal vein occlusion (CRVO) (A), and its corresponding superimposed heat map (B); with CRVO (C), and its corresponding superimposed heat map (D). In the image without CRVO (A), the deep convolution neural network focused on the optic disc (blue), whereas in the image with CRVO (B), the model focused on the optic disc and on the retinal hemorrhages (blue) (D). treatment at an ophthalmic medical center. +e Optos-based telemedicine technology being proposed here could signif- icantly help us in preserving good visual function in CRVO patients living in areas with inadequate ophthalmic care and could potentially be used to cover large areas without ad- equate care facilities. Despite the above, our study has a few limitations. First, we have only compared images of normal retinas with CRVO retinas and did not include images of other retinal diseases. Based on the image examples presented in this study, it may be expected that a CNN algorithm should easily be able to classify between the two types of images. To use this model under clinical conditions, further development and testing to ensure accurate identiﬁcation of multiple conditions other than CRVO would be essential. Addi- tionally, clarity of the eye may decrease in patients with mature cataract or severe vitreous hemorrhage, and in such cases, analysis of images captured using Optos may be diﬃcult. +us, future studies should extensively evaluate the performance and versatility of DL using larger samples and with images of other fundus diseases. 5. Conclusions In conclusion, the DL model performed better than the SVM model in terms of its ability to distinguish between CRVO and normal eyes using ultrawide-ﬁeld fundus ophthalmoscopic images. +is technology has signiﬁcant potential clinical usefulness as it can be combined with telemedicine to reach large areas where no specialist care is available. Data Availability +e Optos image datasets and its corresponding super- imposed heat maps analyzed during the current study are available from the corresponding author on reasonable request. Conflicts of Interest +e authors declare that there are no conﬂicts of interest regarding the publication of this paper. Acknowledgments We thank Masayuki Miki and orthoptists at Tsukazaki Hospital for their support in data collection. References [1] J. W. Yau, P. Lee, T. Y. Wong, J. Best, and A. Jenkins, “Retinal vein occlusion: an approach to diagnosis, systemic risk factors and management,” Internal Medicine Journal, vol. 38, no. 12, pp. 904–910, 2008. [2] S. Rogers, R. L. McIntosh, N. Cheung et al., “+e prevalence of retinal vein occlusion: pooled data from population studies from the United States, Europe, Asia, and Australia,” Oph- thalmology, vol. 117, no. 2, pp. 313–319, 2010. [3] S. S. Hayreh and M. B. Zimmerman, “Fundus changes in central retinal vein occlusion,” Retina, vol. 35, no. 1, pp. 29–42, 2015. [4] S. Yeh, S. J. Kim, A. C. Ho et al., “+erapies for macular edema associated with central retinal vein occlusion: a report by the 6 Journal of Ophthalmology [21] A. K. Akobeng, “Understanding diagnostic tests 3: receiver operating characteristic curves,” Acta Paediatrica, vol. 96, no. 5, pp. 644–647, 2007. [22] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei, “Imagenet: a large-scale hierarchical image database,” in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009. [23] O. Russakovsky, J. Deng, H. Su et al., “Imagenet large scale visual recognition challenge,” International Journal of Com- puter Vision, vol. 115, no. 3, pp. 211–252, 2015. [24] C. Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply- supervised nets,” in Proceedings of 18th International Con- ference on Artiﬁcial Intelligence and Statistics AISTATS, vol. 2, San Diego, CA, USA, 2015. [25] D. Scherer, M. Andreas, and B. Sven, “Evaluation of pooling operations in convolutional architectures for object recog- nition,” in Proceedings of 20th International Artiﬁcial Neural Networks–ICANN, pp. 92–101, +essaloniki, Greece, 2010. [26] J. Redmon, S. Divvala, R. Girshick, and F. Farhadi, “You only look once: uniﬁed, real-time object detection,” arXivpreprint arXiv; 1506.02640, 2015. [27] P. Agrawal, R. Girshick, and J. Malik, “Analyzing the per- formance of multilayer neural networks for object recogni- tion,” in Proceedings of European Conference on Computer Vision, pp. 329–344, Zurich, Switzerland, 2014. [28] N. Qian, “On the momentum term in gradient descent learning algorithms,” Neural Networks, vol. 12, no. 1, pp. 145–151, 1999. [29] Y. Nesterov, “A method for unconstrained convex minimi- zation problem with the rate of convergence O (1/k2),” Doklady AN USSR, vol. 269, pp. 543–547, 1983. [30] R. G. Brereton and G. R. Lloyd, “Support vector machines for classiﬁcation and regression,” Analyst, vol. 135, no. 2, pp. 230–267, 2010. [31] E. F. Schisterman, D. Faraggi, B. Reiser, and J. Hu, “Youden Index and the optimal threshold for markers with mass at zero,” Statistics in Medicine, vol. 27, no. 2, pp. 297–315, 2008. [32] C. Quan, L. Hua, X. Sun, and W. Bai, “Multichannel con- volutional neural network for biological relation extraction,” BioMed Research International, vol. 2016, Article ID 1850404, 10 pages, 2016. [33] S. B. Seong, C. Pae, and H. J. Park, “Geometric convolutional neural network for analyzing surface-based neuroimaging data,” Frontiers in Neuroinformatics, vol. 12, p. 42, 2018. [34] T. Maruyama, N. Hayashi, Y. Sato et al., “Comparison of medical image classiﬁcation accuracy among three machine learning methods,” Journal of X-Ray Science and Technology, pp. 1–9, 2018. [35] H. Wang, Z. Zhou, Y. Li et al., “Comparison of machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer from 18F-FDG PET/ CT images,” EJNMMI Research, vol. 7, no. 1, p. 11, 2017. American Academy of Ophthalmology,” Ophthalmology, vol. 122, no. 4, pp. 769–778, 2015. [5] P. A. Campochiaro, D. M. Brown, C. C. Awh et al., “Sustained beneﬁts from ranibizumab for macular edema following central retinal vein occlusion: twelve-month outcomes of study,” Ophthalmology, vol. 118, no. 10, a phase III pp. 2041–2049, 2011. [6] D. M. Brown, J. S. Heier, W. L. Clark et al., “Intravitreal aﬂibercept injection for macular edema secondary to central retinal vein occlusion: 1-year results from the phase 3 COPERNICUS study,” American Journal of Ophthalmology, vol. 155, no. 3, pp. 429–437, 2013. [7] J. F. Korobelnik, F. G. Holz, J. Roider et al., “Intravitreal aﬂibercept injection for macular edema resulting from central retinal vein occlusion: one-year results of the phase 3 GALILEO study,” Ophthalmology, vol. 121, no. 1, pp. 202–208, 2014. [8] M. Mrsnik, Global Aging 2013: Rising to the Challenge, Standard & Poor’s Rating Services, 2013, https://www.nact. org/resources/2013_NACT_Global_Aging.pdf. [9] A. Nagiel, R. A. Lalane, S. R. Sadda, and S. D. Schwarttz, “Ultra-wide ﬁeld fundus imaging: a review of clinical appli- cations and future trends,” Retina, vol. 36, no. 4, pp. 660–678, 2016. [10] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015. [11] S. Liu, S. Liu, W. Cai et al., “Multimodal neuroimaging feature learning for multiclass diagnosis of Alzheimer’s disease,” IEEE Transactions on Biomedical Engineering, vol. 62, no. 4, pp. 1132–1140, 2015. [12] G. Litjens, C. I. S´anchez, N. Timofeeva et al., “Deep learning as a tool for increased accuracy and eﬃciency of histopatho- logical diagnosis,” Scientiﬁc Reports, vol. 6, no. 1, article 26286, 2016. [13] V. Gulshan, L. Peng, M. Coram et al., “Development and validation of a deep learning algorithm for detection of di- abetic retinopathy in retinal fundus photographs,” JAMA, vol. 316, no. 22, pp. 2402–2410, 2016. [14] W. H. Pinaya, A. Gadelha, O. M. Doyle et al., “Using deep belief network modelling to characterize diﬀerences in brain morphometry in schizophrenia,” Scientiﬁc Reports, vol. 6, no. 1, article 38897, 2016. [15] R. Gargeya and T. Leng, “Automated identiﬁcation of diabetic retinopathy using deep learning,” Ophthalmology, vol. 124, no. 7, pp. 962–969, 2017. [16] H. Ohsugi, H. Tabuchi, H. Enno, and N. Ishitobi, “Accuracy of deep learning, a machine-learning technology, using ultra- wide-ﬁeld fundus ophthalmoscopy for detecting rhegma- togenous retinal detachment,” Scientiﬁc Reports, vol. 7, no. 1, p. 9425, 2017. [17] F. Mosteller and J. W. Tukey, “Data analysis, including sta- tistics,” in Handbook of Social Psychology, Research Methods, G. Lindzey and E. Aronson, Eds., Vol. 2, Addison-Wesley, Reading, MA, USA, 1968. [18] R. Kohavi, “A study of cross-validation and bootstrap for ac- curacy estimation and model selection,” in Proceedings of In- ternational Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1137–1145, Stanford University, Stanford, CA, USA, 1995. [19] D. M. Maslove, T. Podchiyska, and H. J. Lowe, “Discretization of continuous features in clinical datasets,” Journal of the American Medical Informatics Association, vol. 20, no. 3, pp. 544–553, 2013. [20] H. A. Sturges, “+e choice of a class interval,” Journal of the American Statistical Association, vol. 21, no. 153, pp. 65-66, 1926. MEDIATORS INFLAMMATION of Gastroenterology Research and Practice Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Journal of Diabetes Research Disease Markers Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 The Scientific World Journal Hindawi Publishing Corporation Hindawi http://www.hindawi.com www.hindawi.com Volume 2013 Volume 2018 Journal of Immunology Research Hindawi www.hindawi.com Volume 2018 International Journal of Endocrinology Hindawi www.hindawi.com Volume 2018 Submit your manuscripts at www.hindawi.com PPAR Research Hindawi www.hindawi.com Volume 2018 BioMed Research International Hindawi www.hindawi.com Volume 2018 Journal of Obesity Journal of Ophthalmology Stem Cells International Evidence-Based Complementary and Alternative Medicine Journal of Oncology Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2013 Parkinson’s Disease Computational and Mathematical Methods in Medicine Behavioural Neurology AIDS Research and Treatment Oxidative Medicine and Cellular Longevity Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Hindawi www.hindawi.com Volume 2018 Copyright of Journal of Ophthalmology is the property of Hindawi Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.