Research ArticleMachine-Learning Approach to Optimize SMOTE Ratio in ClassImbalance Dataset for Intrusion DetectionJae-Hyun Seo1and Yong-Hyuk Kim21Department of Computer Science and Engineering, Wonkwang University, 460 Iksandae-ro, Iksan-si, Jeonbuk 54649,Republic of Korea2School of Software, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul 01897, Republic of KoreaCorrespondence should be addressed to Yong-Hyuk Kim; yhdfly@kw.ac.krReceived 30 April 2018; Revised 6 August 2018; Accepted 2 October 2018; Published 1 November 2018Academic Editor: Giosu`e LoBoscoCopyright© 2018 Jae-Hyun Seo and Yong-Hyuk Kim. 1is is an open access article distributed under the Creative CommonsAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work isproperly cited.1e KDD CUP 1999 intrusion detection dataset was introduced at the third international knowledge discovery and data miningtools competition, and it has been widely used for many studies. 1e attack types of KDD CUP 1999 dataset are divided into fourcategories: user to root (U2R), remote to local (R2L), denial of service (DoS), and Probe. We use five classes by adding the normalclass. We define the U2R, R2L, and Probe classes, which are each less than 1% of the total dataset, as rare classes. In this study, weattempt to mitigate the class imbalance of the dataset. Using the synthetic minority oversampling technique (SMOTE), weattempted to optimize the SMOTE ratios for the rare classes (U2R, R2L, and Probe). After randomly generating a number of tuplesof SMOTE ratios, these tuples were used to create a numerical model for optimizing the SMOTE ratios of the rare classes. 1esupport vector regression was used to create the model. We assigned each instance in the test dataset to the model and chose thebest SMOTE ratios. 1e experiments using machine-learning techniques were conducted using the best ratios. 1e results usingthe proposed method were significantly better than those of previous approach and other related work.1.Introduction1e early IDS (intrusion detection system) [1] is divided intothe host-based IDS (HIDS) and the network-based IDS(NIDS). HIDS has the advantage of analyzing the system logand resource usage information by the host and user.However, installing an IDS in each host increases themanagement points and wastes more resources. If network-level packet analysis is not possible and the attacker takescontrol of the system, the IDS may be interrupted. NIDShas advantages that it does not need to install an IDS oneach host, and NIDS can perform analysis at the entirenetwork level. However, there is a disadvantage in which it ispossible to confirm only the attack via the IDS, and it isdifficult to confirm the attack attempt at the system level. Inearly 2003, the IDS was losing the trust of users due to theproblem of generating false positives. 1e causes of falsepositives are due to the development of erroneous rules,traffic irregularities, and limitations of pattern matchingtests. Even though the IDS problem has not been solved todate, “pattern matching” is still being used as a basis forsecurity solutions.Intrusion detection attacks [2] are divided into misusedetection and anomaly detection. In misuse detection, de-tected attacks are compared with existing signatures in thedatabase to determine whether they are intrusions. Whilemisuse detection detects only the known attacks, anomalydetection detects a new type of attack that has a patterndifferent from the normal traffic and the known attack types.Many researchers have studied intrusion detection. Ingeneral, researchers attempted to distinguish the normal classfrom attack classes using the publicly available intrusiondetection evaluation dataset and to identify the exact attacktype. However, the classification of rare classes in a huge real-time dataset requires a long computation time, and then it isdifficult to achieve good efficiency. It is necessary to create andtest many experimental datasets to improve classificationperformance by adjusting the class ratio.HindawiComputational Intelligence and NeuroscienceVolume 2018, Article ID 9704672, 11 pageshttps://doi.org/10.1155/2018/9704672In this paper, we present a novel method that optimallyadjusts the SMOTE [3] ratios for rare classes. 1e number ofcases for the tuple of SMOTE ratios is too large to test all thecases. For that reason, we propose the following efficientmethod. We randomly generated some tuples of SMOTEratios and used these tuples to create a model using a supportvector regression (SVR) [4]. We input a number of tuples forSMOTE ratios to the SVR model, and we chose the best tupleof SMOTE ratios. Experimental results using the proposedmethod were significantly better than those of the previousapproach [5].1e contributions we make through the proposed methodare given as follows. We suggest how to find the SMOTEratios that show good performance with very few tests. Hence,we dramatically reduce the amount of computations requiredto find the best SMOTE ratios. We are sure that the proposedmethod is helpful for the study of class imbalances.1e remainder of this paper is organized as follows.Section 2 explains the related works on the KDD CUP 1999dataset [6] and class imbalances. In Section 3, we present thebackground of this research. In Section 4, we suggest a newmethod by creating a numerical model using sampledSMOTE ratios. In Section 5, we explain our experimentalenvironments, procedures, and results. 1e paper ends withour concluding remarks in Section 6.2.Related Work2.1. KDD Dataset.Leung and Leckie [7] studied anomalydetection using unsupervised learning algorithms on theKDD CUP 1999 intrusion detection dataset. 1ese re-searchers proposed density-based clustering and grid-basedclustering algorithms. In density-based clustering, a clusterincludes a minimum number of data points. 1e approachhas the advantage of filtering outliers or finding clusters witharbitrary shapes. In the grid-based approach, all clusteringoperations are conducted on a grid structure. 1e methodhas the advantage of a fast computing speed. With themethod, a classifier can learn from unlabeled data and detectnew types of attacks that were previously unseen. 1e ex-perimental results showed that the accuracy of their methodis similar to one of existing methods, and the method hasseveral advantages in terms of computational complexity.Meng [8] studied intrusion detection machine-learningtechniques on the KDD CUP 1999 dataset. 1ere have beenmany studies using popular methods, such as artificial neuralnetworks, SVM [9], and decision trees. However, thesemethods were rarely used in large-scale real intrusion de-tection systems. 1is researcher aimed at practical anomalydetection and conducted a comparative study with artificialneural networks, SVMs, and decision trees using the sameenvironment as previous studies. In the analysis of theexperimental results, the intrusion detection system withmachine-learning techniques showed a high dependency onthe test environment, and this researcher concluded that itwas important to find a suitable method for applyingmachine-learning techniques to real environments.Davis and Clark [10] reviewed the data preprocessingtechniques  used  in  anomaly-based  network  intrusiondetection systems.1e research focused on network trafficanalysis and feature extraction/selection. Most of studies onNIDS dealt with the TCP/IP packet headers of network traffic.Time-based statistics can be derived from the headers to detectnetwork scans, network worms, and DoS attacks. Recent, fullservice responses are analyzed to detect attacks targetingclients. 1is focuses on which attack classes can be detected bythe reviewed methods. 1is review shows the trends thatscrutinize packets to extract or select the most relevant fea-tures through targeted content parsing. 1ese context-sensitive features are required to detect network attacks.Staudemeyer and Omlin [11] used a long short-termmemory recurrent neural network (LSTM-RNN) to evaluatethe classification performance using the KDD CUP 1999dataset. LSTM networks can learn “memory” and createa model with time series data. 1e LSTM is trained andtested on their modified KDD CUP 1999 dataset. 1e LSTMnetwork structure and parameters were obtained throughexperiments. Several performance measures were used toanalyze experimental results. 1eir results showed thatLSTM-RNN can learn all the unknown attack classes in thetraining dataset. Furthermore, they found that both receiveroperating characteristic (ROC) curves and area under thecurve (AUC) were well suited for evaluating LSTM-RNN.Kim et al. [12] proposed a system-call-language-modelingmethod based on LSTM for designing an anomaly-based hostintrusion detection system. 1ese researchers used an en-semble method to solve the false-alarm rates problem that wascommon in conventional intrusion detection systems. 1emethod can effectively learn the semantic meaning and in-teractions of each system call that existing methods cannothandle. 1ese researchers demonstrated the validity and ef-fectiveness of their method through several tests on publiclyavailable benchmark datasets, and their method has an ad-vantage in that it is easy to transplant to other systems.Kim et al. [13] investigated artificial intelligence intru-sion detection systems that used the deep neural network(DNN) and conducted experiments on the KDD CUP 1999dataset. Data preprocessing (such as data transformationand normalization was conducted) was used to input thedataset into the DNN model. When a learning model wascreated, the DNN was used for data refinement. 1e fulldataset was used to verify the learning model. Performancemeasures, such as the accuracy, detection rate, and false-positive rate, were used to verify the detection efficiency ofthe DNN model, and the model showed good performancefor intrusion detection.Le et al. [14] studied deep-learning algorithms to solvethe problem of machine-learning techniques (such as SVMandk-NN) that had high false-positive rates in intrusiondetection systems. 1ey found six optimizers that are ap-plicable to the LSTM-RNN model to be the best suited forintrusion detection systems. 1e LSTM results using theNadam optimizer were better than previous approaches,with an accuracy of 97.54%, a detection rate of 98.95%, anda false-positive rate of 9.98%. In Table 1, the studies related tointrusion detection are summarized.Seo [5] tried to adjust the class imbalance of train data todetect attacks in the KDD 1999 intrusion dataset. He tested2Computational Intelligence and Neurosciencewith machine-learning algorithms to find efficient SMOTEratios of rare classes such as U2R, R2L, and Probe. Hestudied to improve the performance of classification fo-cusing on detection of rare classes. 1e number of instancesof rare classes in the train data was increased by 12, 9, and1.5 times, respectively. 1e recall metrics ofk-NN tests wereincreased to 0.11 in U2R class and 0.02 in R2L class. 1emetrics of SVM tests were increased to 0.02 in U2R class and0.08 in R2L class, and those of decision tree tests were in-creased to 0.25.2.2. Class Imbalance.In the study of Japkowicz [15],mostpreviously designed concept-learning systems assume thata training dataset is generally well balanced. 1is assumptionis not necessarily correct. In practice, most instances representone class, and only a small number of instances representother ones. 1ese researchers tried to experimentally dem-onstrate that a class imbalance degrades the performance ofstandard classifiers. 1ese researchers compared the perfor-mance of several methods that were previously proposed byother researchers.Japkowicz and Stephen [16] studied class imbalance.Class imbalance has been reported to degrade the perfor-mance of some standard classifiers. 1ey conducted a sys-tematic study by answering the following three problems.First, they attempted to understand the concept complexity,the size of the training set, and the class imbalance level.Second, they discussed several basic resampling or cost-modifying methods to compare the efficiency of the pre-viously proposed class imbalance problems. Finally, theyconducted studies with the assumption that class imbalanceproblems also affected other classification systems, such asdecision trees, neural networks, and SVMs.Chawla et al. [17] studied the SMOTEBoost algorithm. Indata mining, most of the datasets have the class imbalanceproblem, and data mining tools learn from imbalanceddatasets. 1e classifier, which learns from a minority class withvery few instances, tends to be biased towards a high accuracyin the prediction of the majority class. SMOTE is used in thedesign of classifiers to train unbalanced datasets. 1ey pre-sented a new approach to learn from imbalanced datasets bycombining the SMOTE algorithm and the boosting pro-cedure. Unlike standard boosting in which the same weight isgiven to all misclassified examples, SMOTEBoost generatessynthetic examples from minority classes. SMOTEBoost in-directly changes the weight by updating and compensating forthe skewed distribution. In the experiments with SMOTE-Boost applied to several datasets with a high or moderate classimbalance, the classification performance for the minorityclass and the overallF-measure was improved.Drummond and Holte [18] used two commonly usedsampling methods for applying machine learning to im-balanced classes and misclassification costs. 1ey adopteda performance analysis technique called cost curves to ex-plore the interaction of oversampling and undersamplingwith the decision tree classifier C4.5. 1ey showed thatapplying C4.5 to undersampling could establish a reasonablestandard for comparing algorithms. However, it is recom-mended that the cheapest cost classifier becomes a part of thestandard since it can be better than undersampling forrelatively modest costs. Oversampling has little influence onthe sensitivity and the misclassification costs have no sig-nificant effect on performance.Zhou and Liu [19] demonstrated the effect of samplingand threshold-moving in training cost-sensitive neuralnetworks. Both oversampling and undersampling wereconsidered. 1ese techniques modified the distribution oftraining data so that the costs of the instances were explicitlyconveyed by the appearances of the instances. 1reshold-moving moves the output threshold towards inexpensiveclasses to improve classification performance. 1e hard-ensemble and soft-ensemble are used for the experiments.In hard-ensembles and soft-ensembles, all classifiers vote oneach class and return the class that receives the most votes.1e difference between the two ensembles is that hard-ensemble uses binary votes and soft-ensemble uses real-value votes. Twenty-one UCI datasets and actual datasetswere used in their experiments. 1e experimental resultsshowed that as the number of classes increases, the degreeof class imbalance worsens and the efficiency of classifica-tion deteriorates. 1reshold-moving and the soft-ensembleshowed relatively good performance in training cost-sensitive neural networks.Liu et al. [9] used undersampling to solve the class im-balance problem. Undersampling is a very effective method tomitigate class imbalance using only a subset of the majorityclass. 1e disadvantage of the method is that instances ofmajority classes are ignored. 1ey presented two algorithmsto overcome the drawback. First, the EasyEnsemble algorithmsamples several subsets from the majority class, trainsa learner using each subset, and then combines the outputs ofthe learners. EasyEnsemble internally uses the AdaBoostensemble. 1e BalanceCascade algorithm trains learners insequence. At each step, instances of the majority class that arecorrectly classified by the current trained learners are re-moved from further consideration. 1e experimental resultsshowed that both methods produce better solutions than theconventional class imbalance.Burez and Van den Poel [20] attempted to solve the classimbalance problem to predict customer churn. Customerchurn is caused by a customer who changes service provider.Customer churn is a highly rare event in the service industry,but it is a notably interesting and informative research area.Table1: Related work with KDD CUP 1999.AuthorsYearMethodLeung and Leckie [7]   2005Density-based andgrid-based clusteringMeng [8]2011SVM, neural networks,and decision treeDavis and Clark [10]    2011Data preprocessingStaudemeyer andOmlin [11]2013LSTM-RNNKim et al. [12]2016LSTM and ensembleKim et al. [13]2017DNNLe et al. [14]2017DNNSeo [5]2017   SVM,k-NN, and decision treeComputational Intelligence and Neuroscience3However, the class imbalance problem in the context of datamining has not paid it considerable attention until recently.1ey studied how class imbalance can be better handled inchurn prediction. 1ey have conducted studies to improvethe performance of random sampling and undersamplingwith appropriate evaluation matrices, such as AUC and lift.1ey compared gradient boosting, weighted random forestmodeling, and some standard modeling techniques. 1eystudied the performance of both random and advancedundersampling.  1ey  compared  the  specific  modelingtechniques of gradient boosting and weighted randomforests with some standard techniques. In their experiment,the use of undersampling improved the prediction accuracyand the AUC values.Seiffert et al. [21] had stated that class imbalance wasa common problem in various applications. Several techniqueshad been used to mitigate class imbalance problems. 1eyused a hybrid sampling/boosting algorithm called RUSBoostto train skewed training dataset. 1e algorithm was simplerand faster as an alternative of SMOTEBoost. 1ey evaluatedthe  performance  of  RUSBoost,  SMOTEBoost,  randomundersampling, SMOTE, and AdaBoost. 1ey chose fifteendatasets in various applications and then conducted experi-ments with four learners (C4.5D, C4.5N, naive Bayes (NB),and repeated incremental pruning) to produce error reduction(RIPPER) over four evaluation matrices. Both RUSBoost andSMOTEBoost were better than other methods, and RUSBoostperformed equal to or better than SMOTEBoost.Horng et al. [22] proposed an SVM-based intrusiondetection system. 1e system combines a hierarchical clus-tering algorithm, a simple feature selection procedure, andan SVM technique. 1e clustering algorithm provided theSVM with fewer, abstracted, and higher qualified traininginstances. It was able to shorten the training time and improvethe performance of a resultant SVM. 1e obtained SVMmodel could classify the network traffic data more accuratelythrough the simple feature selection procedure. 1e KDDCup 1999 dataset was used to evaluate the proposed system.Compared with other intrusion detection systems that arebased on the same dataset, this system showed better per-formance in the detection of DoS and Probe attacks, and thebest performance in overall accuracy. In Table 2, the studiesrelated to class imbalance are summarized.3.Background3.1. KDD Dataset.1e KDD CUP 1999 dataset [6] used inour experiments is a modification of data generated by theDARPA (Defense Advanced Research Projects Agency)intrusion  detection  evaluation  program  in  1988.  1eDARPA dataset is intercepted data that contain a wide rangeof attacks generated in a military network environment. 1edataset has greatly contributed to the investigation andevaluation of intrusion detection. 1e dataset has beenprepared and managed by MIT’s Lincoln laboratory. In1999, the modified DARPA dataset was used in the KDDCUP 1999 intrusion detection competition. MIT’s Lincolnlaboratory has a similar experimental environment to thetypical U. S. Air Force LAN (local area network). Raw TCPdump data were generated over nine weeks. As in a real AirForce environment, the LAN was activated and variousattacks were executed. However, there was a disadvantage inthat there was no noise in the real data. However, the KDDCUP 1999 dataset served as a testbed to overcome thevulnerabilities of signature-based IDSs in detecting newattack types and attracted the attention of many researchers.1e KDD CUP 1999 dataset is most widely used for theevaluation of such a system. 1ere are many previous ap-proaches using the dataset and it will be possible to comparethe approaches with a new method.Table 3 represents the files in the KDD CUP 1999 datasetand the details for those. 1e files “kddcup.data_10_percent.gz”and “corrected.gz” are used as training data and test data,respectively. 1e training data are compressed binary TCPdump data collected over approximately seven weeks withapproximately 5 million connection records. 1e testing dataare collected over approximately two weeks. 1ey are com-posed of approximately 2 million connection records. Con-nection records are a collection of TCP packets flowing fromthe source IP to the destination IP, and these are classified intoa normal or attack class. In the case of connection recordsbelonging to an attack class, these are represented by exactlyone specific attack type. 1e size of each connection record isapproximately 100 bytes. Attack types are categorized intofour classes, such as DoS, R2L, U2R, and Probe, as shown inTable 4.3.2. SMOTE: Synthetic Minority Oversampling Technique.SMOTE [3] is a method of generating new instances usingexisting ones from rare or minority class. First, we identifythek-nearest neighbors in a class with a small number ofinstances and calculate the differences between a sample andthesekneighbors. We multiply the differences by an arbi-trary value between 0 and 1 and get a resultant value. Next,an instance that is generated using the resultant value isadded to the training data. As a result, SMOTE works byadding any points that slightly move existing instancesaround its neighbors. In the aspect of increasing the numberof instances in rare classes, SMOTE is similar to randomoversampling. However, it does not regenerate the sameinstance.  It  creates  a  new  instance  by  appropriatelyTable2: Related work with class imbalance.AuthorsYearMethodJapkowicz [15]2000Multilayer perceptron (MLP)Japkowicz andStephen [16]2002Decision tree and SVMChawla et al. [17]  2003SMOTE and SMOTEBoostDrummond andHolte [18]2003Decision treeZhou and Liu [19] 2006Cost-sensitive neural networksLiu et al. [9]2009EasyEnsembleBurez and Vanden Poel [20]2009 Gradient boosting and random forestSeiffert et al. [21]   2010RUSBoost, SMOTEBoost,SMOTE, and AdaBoostHonrng et al. [22]  2011    SVM and hierarchical clustering4Computational Intelligence and Neurosciencecombining existing instances, thus making it possible toavoid the disadvantage of overfitting to a certain degree.4.Modeling4.1. Problem Definition.We attempt to maximize classifi-cation performance of the KDD CUP 1999 intrusion de-tection dataset that has class imbalance. 1e dataset hassevere class imbalance. 1erefore, data preprocessing foradjusting the class ratio is required to alleviate the imbal-ance. 1e class imbalance can be adjusted using under-sampling, oversampling, and SMOTE techniques. We usethe SMOTE technique. All tuples of SMOTE ratios should betested to optimize the ratios of each class. However, there aretime and cost constraints to conduct experiments on allcases. 1erefore, we try to find the tuple of SMOTE ratiosthat shows the best performance by experimenting with fewtuples of SMOTE ratios. Formula (1) represents the methodto calculate class imbalance ratio of each class. Figure 1shows the structure of the dataset which is used in theproposed method. Table 5 shows class imbalance ratios ofTrain A,Train Bwhich is the first half ofTrain A,Validationwhich is the second half ofTrain A, andTest.Train Ais theoriginal train data.Train BandValidationin Table 5 arebasically the same.Train Bin Table 5 shows the instancesafter applying the SMOTE ratios in Table 6. We define thethree classes of U2R, R2L, and Probe as rare classes becausethe classes have relatively fewer instances than other classes.Label cardinality ofDis the average number of labels ofthe examples inD:LC(D) �1|D|ｘ|D|i�1Yi，，，，，，，，,imbalance ratioi�Yi，，，，，，，，LC(D)−Yi，，，，，，，，.(1)4.2. Proposed Method.We attempt to optimize the SMOTEratios of rare classes to mitigate the class imbalance. It isdifficult to test all tuples of SMOTE ratios in a short period oftime. 1erefore, we attempt to identify an efficient method witha small number of experiments and reduce computation time.We create an SVR model with a small number of ex-periments and try to get the best tuple of the SMOTE ratiosby inputting enough tuples of SMOTE ratios into the model.We also verify the results through experiments. 1e numbersof 100 and 1,000,000, which are used in the experiments, aredecided by considering computation time and 100 instancesare generated randomly from a uniform distribution. We userandom sampling method instead of grid one. If we can usemore than 100 instances, grid sampling is not bad, but themethod is not appropriate to sample very few instancesuniformly. We set the ranges for the rare classes throughpreliminary experiments, as shown in Table 7.We randomly generate 100 tuples of SMOTE ratioswithin the maximum ranges of Table 7. We conduct ex-periments byinputting the 100 tuples into an SVM clas-sifier. As results, five recall values are given to each of the100 tuples. An SVR model is created using the 100 tuplesand the root mean square of the recall values. We randomlygenerate 1,000,000 tuples of SMOTE ratios and input theminto the SVR model to derive the optimal solution. Weconduct experiments to verify the quality of the best tuple.Formula (2) represents procedure of the proposedmethod. 1e method shows good performance with very fewtests and significantly reduces the amount of computationswhich are required to find the best SMOTE ratios. Figure 2represents its pseudocode.Procedures of the proposed methods as follows:(1) Set the ranges for the rare classes through pre-liminary experiments, as shown in Table 7. 1eranges were searched by inputting successive 2twheretis a nonnegative integer.(2) Generate randomly few tuples of SMOTE ratios froma uniform distribution (independent variable).(3) After drawing recall metrics by giving the tuples intoan SVM classifier, calculate RMS with the metrics(dependent variable).(4) Create an SVR model [4] with the tuples and RMS.(5) Find the best tuple among a lot of tuples, which aregenerated randomly from a uniform distribution,through the SVR model.Table3: Train and test dataset with labels of KDD CUP 1999dataset.DatasetDetailsKddcup.data.gzTraining dataset (743 MB)Kddcup.data_10_percent.gz(23 attack types)10% subset of trainingdataset (75 MB)Corrected.gz (23 attack types)Test dataset (45 MB)Table4: Five main categories of KDD CUP 1999 dataset.AttacksDescriptionsNormalNormal trafficDoSDenial of service, e.g., syn floodR2LUnauthorized access from a remote machine,e.g., guessing passwordU2RUnauthorized access to local superuser (root)privileges, e.g., various “buffer overflow” attacksProbingSurveillance and other probing, e.g., port scanningF]]dataset#est#rain3z#rain3x–alidation50%50%Figure1: 1e structure of the dataset used in the proposed method.Computational Intelligence and Neuroscience5// N: normal,U: change to U2R,R: R2L,D: DoS,P: change to Probe,m: the number of classes,// Umax,Rmax,Pmax: maximum range of each class,// Tmodel: tuples of SMOTE ratios required formodel creation,// Teval: tuples of SMOTE ratios required to evaluatethe model,// tbest: the best tuple amongTeval,U⟵1,Umax＂＃,R⟵1,Rmax＂＃,P⟵1,Pmax＂＃,Tmodel⟵U,R,P{   },RMS���������������������������ｐmi�0SVMClassifierTmodel